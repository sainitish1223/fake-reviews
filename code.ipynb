{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Embedding, LSTM, Dense, Dropout,\n",
        "    Layer, MultiHeadAttention, LayerNormalization,\n",
        "    Concatenate, GlobalAveragePooling1D\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "class UserProductInteractionLayer(Layer):\n",
        "    \"\"\"\n",
        "    Custom layer for capturing user-product interaction patterns\n",
        "    \"\"\"\n",
        "    def __init__(self, units, num_heads=4, **kwargs):\n",
        "        super(UserProductInteractionLayer, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Implement multi-head attention mechanism\n",
        "        self.multi_head_attention = MultiHeadAttention(\n",
        "            num_heads=self.num_heads,\n",
        "            key_dim=self.units\n",
        "        )\n",
        "        self.layer_norm = LayerNormalization()\n",
        "        super(UserProductInteractionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Assume inputs are [text_embedding, user_embedding, product_embedding]\n",
        "        text_embed, user_embed, product_embed = inputs\n",
        "\n",
        "        # Interactive attention\n",
        "        attention_output = self.multi_head_attention(\n",
        "            query=text_embed,\n",
        "            value=user_embed,\n",
        "            key=product_embed\n",
        "        )\n",
        "\n",
        "        # Residual connection and layer normalization\n",
        "        interaction_output = self.layer_norm(text_embed + attention_output)\n",
        "\n",
        "        return interaction_output\n",
        "\n",
        "class MIANAReviewPredictor:\n",
        "    def __init__(self, max_words=5000, max_len=100):\n",
        "        # Create necessary directories\n",
        "        self.base_dir = 'weights'\n",
        "        os.makedirs(self.base_dir, exist_ok=True)\n",
        "        os.makedirs('metrics', exist_ok=True)\n",
        "\n",
        "        self.max_words = max_words\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = Tokenizer(num_words=max_words)\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        # Tokenize text\n",
        "        self.tokenizer.fit_on_texts(df['text_'])\n",
        "        text_sequences = self.tokenizer.texts_to_sequences(df['text_'])\n",
        "        text_padded = pad_sequences(text_sequences, maxlen=self.max_len)\n",
        "\n",
        "        # Encode labels\n",
        "        labels = self.label_encoder.fit_transform(df['label'])\n",
        "\n",
        "        return text_padded, labels\n",
        "\n",
        "    def create_miana_model(self, vocab_size, num_classes):\n",
        "        # Text input\n",
        "        text_input = Input(shape=(self.max_len,), name='text_input')\n",
        "\n",
        "        # Embedding layer\n",
        "        embedding = Embedding(\n",
        "            vocab_size,\n",
        "            256,\n",
        "            input_length=self.max_len\n",
        "        )(text_input)\n",
        "\n",
        "        # Word-Level Fusion Module (LSTM-based)\n",
        "        lstm1 = LSTM(256, return_sequences=True, name='word_level_lstm1')(embedding)\n",
        "        lstm2 = LSTM(128, return_sequences=True, name='word_level_lstm2')(lstm1)\n",
        "\n",
        "        # User-Product Interaction Layer\n",
        "        interaction_layer = UserProductInteractionLayer(\n",
        "            units=128,\n",
        "            num_heads=4\n",
        "        )([lstm2, lstm2, lstm2])  # Using same embedding for demonstration\n",
        "\n",
        "        # Sentence-Level Interactive Attention\n",
        "        multi_head_attention = MultiHeadAttention(\n",
        "            num_heads=4,\n",
        "            key_dim=128\n",
        "        )(query=interaction_layer, value=interaction_layer)\n",
        "\n",
        "        # Pooling and Dropout\n",
        "        pooled = GlobalAveragePooling1D()(multi_head_attention)\n",
        "        dropout = Dropout(0.5)(pooled)\n",
        "\n",
        "        # Output layer\n",
        "        output = Dense(num_classes, activation='softmax', name='output')(dropout)\n",
        "\n",
        "        # Compile the model\n",
        "        model = Model(inputs=text_input, outputs=output)\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_and_save(self, df, test_size=0.2, epochs=5, batch_size=32):\n",
        "        try:\n",
        "            # Preprocess data\n",
        "            X, y = self.preprocess_data(df)\n",
        "\n",
        "            # Split data\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=test_size, random_state=42\n",
        "            )\n",
        "\n",
        "            # Create MIANA model\n",
        "            vocab_size = len(self.tokenizer.word_index) + 1\n",
        "            num_classes = len(np.unique(y))\n",
        "\n",
        "            model = self.create_miana_model(vocab_size, num_classes)\n",
        "\n",
        "            # Training\n",
        "            history = model.fit(\n",
        "                X_train, y_train,\n",
        "                validation_data=(X_test, y_test),\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            # Ensure correct weight file naming\n",
        "            weights_path = os.path.join(self.base_dir, 'miana_model.weights.h5')\n",
        "            model.save_weights(weights_path)\n",
        "\n",
        "            # Save additional artifacts\n",
        "            import joblib\n",
        "            import pickle\n",
        "\n",
        "            # Save label encoder\n",
        "            label_encoder_path = os.path.join(self.base_dir, 'label_encoder.joblib')\n",
        "            joblib.dump(self.label_encoder, label_encoder_path)\n",
        "\n",
        "            # Save tokenizer\n",
        "            tokenizer_path = os.path.join(self.base_dir, 'tokenizer.pickle')\n",
        "            with open(tokenizer_path, 'wb') as handle:\n",
        "                pickle.dump(self.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "            # Generate metrics visualization\n",
        "            self._generate_metrics_visualization(history)\n",
        "\n",
        "            print(f\"Model weights saved to: {weights_path}\")\n",
        "            print(f\"Label encoder saved to: {label_encoder_path}\")\n",
        "            print(f\"Tokenizer saved to: {tokenizer_path}\")\n",
        "\n",
        "            return model, history\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Training error: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None, None\n",
        "\n",
        "    def _generate_metrics_visualization(self, history):\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        # Loss subplot\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title('MIANA Model Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        # Accuracy subplot\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "        plt.title('MIANA Model Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('metrics/miana_training_metrics.png')\n",
        "        plt.close()\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Suppress TensorFlow warnings\n",
        "    import os\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "    # Load reviews dataset\n",
        "    df = pd.read_csv('/content/reviews.csv')  # Replace with your actual dataset\n",
        "\n",
        "    # Initialize MIANA predictor\n",
        "    miana_predictor = MIANAReviewPredictor()\n",
        "\n",
        "    # Train and save the MIANA model\n",
        "    miana_predictor.train_and_save(df)\n",
        "\n",
        "    print(\"MIANA Model training completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXcO7oaxGX6f",
        "outputId": "7c8f6f96-006d-471a-fbaa-fbdee2d7b99c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 33ms/step - accuracy: 0.8243 - loss: 0.3708 - val_accuracy: 0.9236 - val_loss: 0.2035\n",
            "Epoch 2/5\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 33ms/step - accuracy: 0.9475 - loss: 0.1442 - val_accuracy: 0.9442 - val_loss: 0.1538\n",
            "Epoch 3/5\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.9671 - loss: 0.0912 - val_accuracy: 0.9369 - val_loss: 0.1618\n",
            "Epoch 4/5\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.9770 - loss: 0.0642 - val_accuracy: 0.9405 - val_loss: 0.1618\n",
            "Epoch 5/5\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 32ms/step - accuracy: 0.9851 - loss: 0.0444 - val_accuracy: 0.9393 - val_loss: 0.2600\n",
            "Model weights saved to: weights/miana_model.weights.h5\n",
            "Label encoder saved to: weights/label_encoder.joblib\n",
            "Tokenizer saved to: weights/tokenizer.pickle\n",
            "MIANA Model training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Embedding, LSTM, Dense, Dropout,\n",
        "    Layer, MultiHeadAttention, LayerNormalization,\n",
        "    Concatenate, GlobalAveragePooling1D\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    precision_recall_curve,\n",
        "    roc_curve,\n",
        "    auc\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "class UserProductInteractionLayer(Layer):\n",
        "    \"\"\"\n",
        "    Custom layer for capturing user-product interaction patterns\n",
        "    \"\"\"\n",
        "    def __init__(self, units, num_heads=4, **kwargs):\n",
        "        super(UserProductInteractionLayer, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Implement multi-head attention mechanism\n",
        "        self.multi_head_attention = MultiHeadAttention(\n",
        "            num_heads=self.num_heads,\n",
        "            key_dim=self.units\n",
        "        )\n",
        "        self.layer_norm = LayerNormalization()\n",
        "        super(UserProductInteractionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Assume inputs are [text_embedding, user_embedding, product_embedding]\n",
        "        text_embed, user_embed, product_embed = inputs\n",
        "\n",
        "        # Interactive attention\n",
        "        attention_output = self.multi_head_attention(\n",
        "            query=text_embed,\n",
        "            value=user_embed,\n",
        "            key=product_embed\n",
        "        )\n",
        "\n",
        "        # Residual connection and layer normalization\n",
        "        interaction_output = self.layer_norm(text_embed + attention_output)\n",
        "\n",
        "        return interaction_output\n",
        "\n",
        "class MIANAReviewPredictor:\n",
        "    def __init__(self, max_words=5000, max_len=100):\n",
        "        # Create necessary directories\n",
        "        self.base_dir = 'weights'\n",
        "        os.makedirs(self.base_dir, exist_ok=True)\n",
        "        os.makedirs('metrics', exist_ok=True)\n",
        "\n",
        "        self.max_words = max_words\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = Tokenizer(num_words=max_words)\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        # Tokenize text\n",
        "        self.tokenizer.fit_on_texts(df['text_'])\n",
        "        text_sequences = self.tokenizer.texts_to_sequences(df['text_'])\n",
        "        text_padded = pad_sequences(text_sequences, maxlen=self.max_len)\n",
        "\n",
        "        # Encode labels\n",
        "        labels = self.label_encoder.fit_transform(df['label'])\n",
        "\n",
        "        return text_padded, labels\n",
        "\n",
        "    def create_miana_model(self, vocab_size, num_classes):\n",
        "        # Text input\n",
        "        text_input = Input(shape=(self.max_len,), name='text_input')\n",
        "\n",
        "        # Embedding layer\n",
        "        embedding = Embedding(\n",
        "            vocab_size,\n",
        "            256,\n",
        "            input_length=self.max_len\n",
        "        )(text_input)\n",
        "\n",
        "        # Word-Level Fusion Module (LSTM-based)\n",
        "        lstm1 = LSTM(256, return_sequences=True, name='word_level_lstm1')(embedding)\n",
        "        lstm2 = LSTM(128, return_sequences=True, name='word_level_lstm2')(lstm1)\n",
        "\n",
        "        # User-Product Interaction Layer\n",
        "        interaction_layer = UserProductInteractionLayer(\n",
        "            units=128,\n",
        "            num_heads=4\n",
        "        )([lstm2, lstm2, lstm2])  # Using same embedding for demonstration\n",
        "\n",
        "        # Sentence-Level Interactive Attention\n",
        "        multi_head_attention = MultiHeadAttention(\n",
        "            num_heads=4,\n",
        "            key_dim=128\n",
        "        )(query=interaction_layer, value=interaction_layer)\n",
        "\n",
        "        # Pooling and Dropout\n",
        "        pooled = GlobalAveragePooling1D()(multi_head_attention)\n",
        "        dropout = Dropout(0.5)(pooled)\n",
        "\n",
        "        # Output layer\n",
        "        output = Dense(num_classes, activation='softmax', name='output')(dropout)\n",
        "\n",
        "        # Compile the model\n",
        "        model = Model(inputs=text_input, outputs=output)\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_and_save(self, df, test_size=0.2, epochs=5, batch_size=32):\n",
        "        try:\n",
        "            # Preprocess data\n",
        "            X, y = self.preprocess_data(df)\n",
        "\n",
        "            # Split data\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                X, y, test_size=test_size, random_state=42\n",
        "            )\n",
        "\n",
        "            # Create MIANA model\n",
        "            vocab_size = len(self.tokenizer.word_index) + 1\n",
        "            num_classes = len(np.unique(y))\n",
        "\n",
        "            model = self.create_miana_model(vocab_size, num_classes)\n",
        "\n",
        "            # Training\n",
        "            history = model.fit(\n",
        "                X_train, y_train,\n",
        "                validation_data=(X_test, y_test),\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            # Ensure correct weight file naming\n",
        "            weights_path = os.path.join(self.base_dir, 'miana_model.weights.h5')\n",
        "            model.save_weights(weights_path)\n",
        "\n",
        "            # Save additional artifacts\n",
        "            # Save label encoder\n",
        "            label_encoder_path = os.path.join(self.base_dir, 'label_encoder.joblib')\n",
        "            joblib.dump(self.label_encoder, label_encoder_path)\n",
        "\n",
        "            # Save tokenizer\n",
        "            tokenizer_path = os.path.join(self.base_dir, 'tokenizer.pickle')\n",
        "            with open(tokenizer_path, 'wb') as handle:\n",
        "                pickle.dump(self.tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "            # Generate comprehensive metrics visualization\n",
        "            self._generate_comprehensive_metrics(model, X_test, y_test, history)\n",
        "\n",
        "            print(f\"Model weights saved to: {weights_path}\")\n",
        "            print(f\"Label encoder saved to: {label_encoder_path}\")\n",
        "            print(f\"Tokenizer saved to: {tokenizer_path}\")\n",
        "\n",
        "            return model, history\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Training error: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None, None\n",
        "\n",
        "    def _generate_comprehensive_metrics(self, model, X_test, y_test, history):\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        # Predict on test data\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "        # 1. Training History Metrics\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        plt.suptitle('MIANA Model Training Analysis', fontsize=16)\n",
        "\n",
        "        # Loss subplot\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        # Accuracy subplot\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "        plt.title('Training and Validation Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "\n",
        "        # 2. Confusion Matrix\n",
        "        plt.subplot(2, 2, 3)\n",
        "        cm = confusion_matrix(y_test, y_pred_classes)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "\n",
        "        # 3. Classification Report\n",
        "        plt.subplot(2, 2, 4)\n",
        "        class_report = classification_report(\n",
        "            y_test,\n",
        "            y_pred_classes,\n",
        "            target_names=self.label_encoder.classes_,\n",
        "            output_dict=True\n",
        "        )\n",
        "        report_data = []\n",
        "        for key, row in class_report.items():\n",
        "            if isinstance(row, dict):\n",
        "                report_data.append([\n",
        "                    key,\n",
        "                    round(row['precision'], 2),\n",
        "                    round(row['recall'], 2),\n",
        "                    round(row['f1-score'], 2)\n",
        "                ])\n",
        "\n",
        "        plt.axis('off')\n",
        "        plt.table(\n",
        "            cellText=report_data,\n",
        "            colLabels=['Class', 'Precision', 'Recall', 'F1-Score'],\n",
        "            loc='center',\n",
        "            cellLoc='center'\n",
        "        )\n",
        "        plt.title('Classification Report')\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.savefig('metrics/miana_comprehensive_metrics.png')\n",
        "        plt.close()\n",
        "\n",
        "        # 4. ROC Curves and Precision-Recall Curves\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # ROC Curve\n",
        "        plt.subplot(1, 2, 1)\n",
        "        # One-vs-Rest ROC\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "        for i in range(len(self.label_encoder.classes_)):\n",
        "            fpr[i], tpr[i], _ = roc_curve(\n",
        "                (y_test == i).ravel(),\n",
        "                y_pred[:, i].ravel()\n",
        "            )\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "            plt.plot(\n",
        "                fpr[i], tpr[i],\n",
        "                label=f'ROC curve (class {self.label_encoder.classes_[i]}, area = {roc_auc[i]:.2f})'\n",
        "            )\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "\n",
        "        # Precision-Recall Curve\n",
        "        plt.subplot(1, 2, 2)\n",
        "        for i in range(len(self.label_encoder.classes_)):\n",
        "            precision, recall, _ = precision_recall_curve(\n",
        "                (y_test == i).ravel(),\n",
        "                y_pred[:, i].ravel()\n",
        "            )\n",
        "            plt.plot(\n",
        "                recall, precision,\n",
        "                label=f'Precision-Recall curve (class {self.label_encoder.classes_[i]})'\n",
        "            )\n",
        "        plt.xlabel('Recall')\n",
        "        plt.ylabel('Precision')\n",
        "        plt.title('Precision-Recall Curve')\n",
        "        plt.legend(loc=\"lower left\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('metrics/miana_roc_pr_curves.png')\n",
        "        plt.close()\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Suppress TensorFlow warnings\n",
        "    import os\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "    # Load reviews dataset\n",
        "    df = pd.read_csv('/content/reviews.csv')  # Replace with your actual dataset\n",
        "\n",
        "    # Initialize MIANA predictor\n",
        "    miana_predictor = MIANAReviewPredictor()\n",
        "\n",
        "    # Train and save the MIANA model\n",
        "    miana_predictor.train_and_save(df)\n",
        "\n",
        "    print(\"MIANA Model training completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xczl4LcvKOFO",
        "outputId": "95413975-ad92-4102-d69b-599a2e3d6a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 35ms/step - accuracy: 0.8297 - loss: 0.3640 - val_accuracy: 0.9258 - val_loss: 0.1839\n",
            "Epoch 2/5\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 33ms/step - accuracy: 0.9443 - loss: 0.1483 - val_accuracy: 0.9389 - val_loss: 0.1687\n",
            "Epoch 3/5\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9666 - loss: 0.0921 - val_accuracy: 0.9437 - val_loss: 0.1558\n",
            "Epoch 4/5\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9748 - loss: 0.0711 - val_accuracy: 0.9427 - val_loss: 0.1835\n",
            "Epoch 5/5\n",
            "\u001b[1m1011/1011\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 34ms/step - accuracy: 0.9820 - loss: 0.0483 - val_accuracy: 0.9402 - val_loss: 0.2159\n",
            "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n",
            "Model weights saved to: weights/miana_model.weights.h5\n",
            "Label encoder saved to: weights/label_encoder.joblib\n",
            "Tokenizer saved to: weights/tokenizer.pickle\n",
            "MIANA Model training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "class MIANAReviewInference:\n",
        "    def __init__(self, base_dir='weights'):\n",
        "        \"\"\"\n",
        "        Initialize the inference model with pre-trained weights and preprocessing artifacts\n",
        "\n",
        "        Args:\n",
        "            base_dir (str): Directory containing saved model artifacts\n",
        "        \"\"\"\n",
        "        self.base_dir = base_dir\n",
        "\n",
        "        # Load tokenizer\n",
        "        tokenizer_path = os.path.join(base_dir, 'tokenizer.pickle')\n",
        "        with open(tokenizer_path, 'rb') as handle:\n",
        "            self.tokenizer = pickle.load(handle)\n",
        "\n",
        "        # Load label encoder\n",
        "        label_encoder_path = os.path.join(base_dir, 'label_encoder.joblib')\n",
        "        self.label_encoder = joblib.load(label_encoder_path)\n",
        "\n",
        "        # Model configuration parameters\n",
        "        self.max_len = 100  # Must match training configuration\n",
        "        self.max_words = 5000  # Must match training configuration\n",
        "\n",
        "        # Create and load model\n",
        "        self.model = self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"\n",
        "        Recreate and load pre-trained model weights\n",
        "\n",
        "        Returns:\n",
        "            Loaded Keras model with pre-trained weights\n",
        "        \"\"\"\n",
        "        from tensorflow.keras.layers import (\n",
        "            Input, Embedding, LSTM, Dense, Dropout,\n",
        "            Layer, MultiHeadAttention, LayerNormalization,\n",
        "            GlobalAveragePooling1D\n",
        "        )\n",
        "        from tensorflow.keras.models import Model\n",
        "\n",
        "        class UserProductInteractionLayer(Layer):\n",
        "            def __init__(self, units, num_heads=4, **kwargs):\n",
        "                super(UserProductInteractionLayer, self).__init__(**kwargs)\n",
        "                self.units = units\n",
        "                self.num_heads = num_heads\n",
        "\n",
        "            def build(self, input_shape):\n",
        "                self.multi_head_attention = MultiHeadAttention(\n",
        "                    num_heads=self.num_heads,\n",
        "                    key_dim=self.units\n",
        "                )\n",
        "                self.layer_norm = LayerNormalization()\n",
        "                super(UserProductInteractionLayer, self).build(input_shape)\n",
        "\n",
        "            def call(self, inputs):\n",
        "                text_embed, user_embed, product_embed = inputs\n",
        "                attention_output = self.multi_head_attention(\n",
        "                    query=text_embed,\n",
        "                    value=user_embed,\n",
        "                    key=product_embed\n",
        "                )\n",
        "                interaction_output = self.layer_norm(text_embed + attention_output)\n",
        "                return interaction_output\n",
        "\n",
        "        # Recreate model architecture\n",
        "        text_input = Input(shape=(self.max_len,), name='text_input')\n",
        "\n",
        "        embedding = Embedding(\n",
        "            len(self.tokenizer.word_index) + 1,\n",
        "            256,\n",
        "            input_length=self.max_len\n",
        "        )(text_input)\n",
        "\n",
        "        lstm1 = LSTM(256, return_sequences=True, name='word_level_lstm1')(embedding)\n",
        "        lstm2 = LSTM(128, return_sequences=True, name='word_level_lstm2')(lstm1)\n",
        "\n",
        "        interaction_layer = UserProductInteractionLayer(\n",
        "            units=128,\n",
        "            num_heads=4\n",
        "        )([lstm2, lstm2, lstm2])\n",
        "\n",
        "        multi_head_attention = MultiHeadAttention(\n",
        "            num_heads=4,\n",
        "            key_dim=128\n",
        "        )(query=interaction_layer, value=interaction_layer)\n",
        "\n",
        "        pooled = GlobalAveragePooling1D()(multi_head_attention)\n",
        "        dropout = Dropout(0.5)(pooled)\n",
        "\n",
        "        output = Dense(len(self.label_encoder.classes_), activation='softmax', name='output')(dropout)\n",
        "\n",
        "        model = Model(inputs=text_input, outputs=output)\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        # Load pre-trained weights\n",
        "        weights_path = os.path.join(self.base_dir, 'miana_model.weights.h5')\n",
        "        model.load_weights(weights_path)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Preprocess input text for model prediction\n",
        "\n",
        "        Args:\n",
        "            text (str): Input review text\n",
        "\n",
        "        Returns:\n",
        "            Preprocessed and padded text sequence\n",
        "        \"\"\"\n",
        "        # Tokenize and pad the text\n",
        "        text_sequence = self.tokenizer.texts_to_sequences([text])\n",
        "        text_padded = pad_sequences(\n",
        "            text_sequence,\n",
        "            maxlen=self.max_len,\n",
        "            padding='post',\n",
        "            truncating='post'\n",
        "        )\n",
        "\n",
        "        return text_padded\n",
        "\n",
        "    def predict(self, review_text, top_k=3):\n",
        "        \"\"\"\n",
        "        Predict label for the given review text\n",
        "\n",
        "        Args:\n",
        "            review_text (str): Input review text\n",
        "            top_k (int): Number of top predictions to return\n",
        "\n",
        "        Returns:\n",
        "            List of top predictions with probabilities\n",
        "        \"\"\"\n",
        "        # Preprocess text\n",
        "        processed_text = self.preprocess_text(review_text)\n",
        "\n",
        "        # Predict probabilities\n",
        "        prediction = self.model.predict(processed_text)[0]\n",
        "\n",
        "        # Get top-k predictions\n",
        "        top_indices = prediction.argsort()[-top_k:][::-1]\n",
        "        top_predictions = [\n",
        "            {\n",
        "                'label': self.label_encoder.classes_[idx],\n",
        "                'probability': float(prediction[idx])\n",
        "            }\n",
        "            for idx in top_indices\n",
        "        ]\n",
        "\n",
        "        return top_predictions\n",
        "\n",
        "def main():\n",
        "    # Example usage\n",
        "    inference_model = MIANAReviewInference()\n",
        "\n",
        "    # Example reviews\n",
        "    reviews = [\n",
        "        \"This product is amazing and works perfectly!\",\n",
        "        \"I'm very disappointed with the quality of this item.\",\n",
        "        \"Decent product, but could use some improvements.\"\n",
        "    ]\n",
        "\n",
        "    # Predict for each review\n",
        "    for review in reviews:\n",
        "        predictions = inference_model.predict(review)\n",
        "        print(f\"\\nReview: {review}\")\n",
        "        print(\"Top Predictions:\")\n",
        "        for pred in predictions:\n",
        "            print(f\"- {pred['label']}: {pred['probability']:.2%}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiL0XUh0K3ck",
        "outputId": "2d374014-ebeb-43e8-bb9f-9f747208b353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 56 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 790ms/step\n",
            "\n",
            "Review: This product is amazing and works perfectly!\n",
            "Top Predictions:\n",
            "- OR: 96.11%\n",
            "- CG: 3.89%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
            "\n",
            "Review: I'm very disappointed with the quality of this item.\n",
            "Top Predictions:\n",
            "- CG: 99.92%\n",
            "- OR: 0.08%\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
            "\n",
            "Review: Decent product, but could use some improvements.\n",
            "Top Predictions:\n",
            "- OR: 99.98%\n",
            "- CG: 0.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9AMvoInzOP5t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
